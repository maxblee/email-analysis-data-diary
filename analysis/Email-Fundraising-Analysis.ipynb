{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hickenlooper and Gardner E-Mail Fundraising Analysis\n",
    "\n",
    "How do John Hickenlooper and Cory Gardner message themselves to potential donors? \n",
    "\n",
    "That's the question I wanted to find out. Based on [political science research](https://www.tandfonline.com/eprint/g6tVMXwGRKZZQbNmYusB/full), political candidates tend to message themselves in more partisan ways and ask for money more often in e-mails than they do in, say, TV ads. \n",
    "\n",
    "Additionally, based on reading the e-mails, you can spot some stylistic differences between Gardner's e-mails and Hickenlooper's e-mails. So I wanted to see how we can use quantitative methods to document how the two candidates are messaging themselves in political fundraising e-mails.\n",
    "\n",
    "## Set-up\n",
    "\n",
    "First, I'm going to load in the data I prepared using the code in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>valid_message</th>\n",
       "      <th>slug</th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>html</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gardner</td>\n",
       "      <td>True</td>\n",
       "      <td>gardner-206</td>\n",
       "      <td>Re-upping this in your inbox</td>\n",
       "      <td>info@coryforco.com</td>\n",
       "      <td>2019-11-20 08:15:59-07:00</td>\n",
       "      <td>Re-upping this in your inbox in case you happe...</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n\\t&lt;style type...</td>\n",
       "      <td>Re-upping this in your inbox. Re-upping this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gardner</td>\n",
       "      <td>True</td>\n",
       "      <td>gardner-254</td>\n",
       "      <td>Your Quarterly Status Update</td>\n",
       "      <td>info@coryforco.com</td>\n",
       "      <td>2019-09-28 08:15:40-06:00</td>\n",
       "      <td>Will you step up now to defend the Senate next...</td>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 S...</td>\n",
       "      <td>Your Quarterly Status Update. Will you step up...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  candidate  valid_message         slug                       subject  \\\n",
       "0   gardner           True  gardner-206  Re-upping this in your inbox   \n",
       "1   gardner           True  gardner-254  Your Quarterly Status Update   \n",
       "\n",
       "                 from                       date  \\\n",
       "0  info@coryforco.com  2019-11-20 08:15:59-07:00   \n",
       "1  info@coryforco.com  2019-09-28 08:15:40-06:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  Re-upping this in your inbox in case you happe...   \n",
       "1  Will you step up now to defend the Senate next...   \n",
       "\n",
       "                                                html  \\\n",
       "0  <!doctype html>\\n<html>\\n<head>\\n\\t<style type...   \n",
       "1  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 S...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Re-upping this in your inbox. Re-upping this i...  \n",
       "1  Your Quarterly Status Update. Will you step up...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import sys; sys.path.append(\"..\")\n",
    "\n",
    "COLORADO = \"America/Denver\"\n",
    "fundraising = pd.read_csv(\"../data/clean/fundraising_emails.csv\", index_col=0)\n",
    "fundraising[\"date\"] = pd.to_datetime(fundraising.date)\n",
    "fundraising.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics\n",
    "\n",
    "Now, I'm going to compute some basic statistics about the data. I wound up using `describe` in `pandas` and two function calls to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>gardner</th>\n",
       "      <th>hickenlooper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">valid_message</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">slug</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>gardner-202</td>\n",
       "      <td>hickenlooper-464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">subject</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>302</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>We can’t afford to slip now</td>\n",
       "      <td>Short note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">from</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>info@coryforco.com</td>\n",
       "      <td>hello@hickenlooper.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>277</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">date</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2019-08-28 11:15:54-06:00</td>\n",
       "      <td>2020-06-10 11:27:51-06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">text</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>We need your immediate support to hit our Q1 F...</td>\n",
       "      <td>Today is just the Mondayest of Mondays, ain’t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">html</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n\\t&lt;style type...</td>\n",
       "      <td>&lt;html&gt;\\r\\n&lt;head&gt;\\r\\n\\t&lt;title&gt;&lt;/title&gt;\\r\\n\\t&lt;me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">clean_text</th>\n",
       "      <th>count</th>\n",
       "      <td>309</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>305</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>FWD: FEC Deadline update. Cory Gardner knows a...</td>\n",
       "      <td>Honesty. Folks, Here's the honest truth we wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "candidate                                                       gardner  \\\n",
       "valid_message count                                                 309   \n",
       "              unique                                                  1   \n",
       "              top                                                  True   \n",
       "              freq                                                  309   \n",
       "slug          count                                                 309   \n",
       "              unique                                                309   \n",
       "              top                                           gardner-202   \n",
       "              freq                                                    1   \n",
       "subject       count                                                 309   \n",
       "              unique                                                302   \n",
       "              top                           We can’t afford to slip now   \n",
       "              freq                                                    2   \n",
       "from          count                                                 309   \n",
       "              unique                                                  9   \n",
       "              top                                    info@coryforco.com   \n",
       "              freq                                                  277   \n",
       "date          count                                                 309   \n",
       "              unique                                                309   \n",
       "              top                             2019-08-28 11:15:54-06:00   \n",
       "              freq                                                    1   \n",
       "text          count                                                 309   \n",
       "              unique                                                309   \n",
       "              top     We need your immediate support to hit our Q1 F...   \n",
       "              freq                                                    1   \n",
       "html          count                                                 309   \n",
       "              unique                                                309   \n",
       "              top     <!doctype html>\\n<html>\\n<head>\\n\\t<style type...   \n",
       "              freq                                                    1   \n",
       "clean_text    count                                                 309   \n",
       "              unique                                                305   \n",
       "              top     FWD: FEC Deadline update. Cory Gardner knows a...   \n",
       "              freq                                                    2   \n",
       "\n",
       "candidate                                                  hickenlooper  \n",
       "valid_message count                                                 375  \n",
       "              unique                                                  1  \n",
       "              top                                                  True  \n",
       "              freq                                                  375  \n",
       "slug          count                                                 375  \n",
       "              unique                                                375  \n",
       "              top                                      hickenlooper-464  \n",
       "              freq                                                    1  \n",
       "subject       count                                                 375  \n",
       "              unique                                                361  \n",
       "              top                                            Short note  \n",
       "              freq                                                    4  \n",
       "from          count                                                 375  \n",
       "              unique                                                  3  \n",
       "              top                                hello@hickenlooper.com  \n",
       "              freq                                                  315  \n",
       "date          count                                                 375  \n",
       "              unique                                                375  \n",
       "              top                             2020-06-10 11:27:51-06:00  \n",
       "              freq                                                    1  \n",
       "text          count                                                 375  \n",
       "              unique                                                375  \n",
       "              top     Today is just the Mondayest of Mondays, ain’t ...  \n",
       "              freq                                                    1  \n",
       "html          count                                                 375  \n",
       "              unique                                                375  \n",
       "              top     <html>\\r\\n<head>\\r\\n\\t<title></title>\\r\\n\\t<me...  \n",
       "              freq                                                    1  \n",
       "clean_text    count                                                 375  \n",
       "              unique                                                375  \n",
       "              top     Honesty. Folks, Here's the honest truth we wan...  \n",
       "              freq                                                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fundraising.groupby(\"candidate\").describe(datetime_is_numeric=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 8, 22, 7, 54, 5, tzinfo=tzoffset(None, -21600))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fundraising.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 7, 16, 9, 51, 14, tzinfo=tzoffset(None, -21600))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fundraising.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "From here, I loaded in indexes showing the number of times different words or phrases appeared in Hickenlooper's and Gardner's emails. This code does two things. First of all, it \"tokenizes\" the emails, converting each document from a string to a list of strings roughly representing words. More specifically, it tokenizes the documents using a series of regular expressions and using WordNet with a Part of Speech tagger. That converts certain words like \"dogs\" to their root form (\"dog\" in this case), but it does so based on the part of speech of the word. For some types of words like prices that computers can't really make sense of, especially using the methods I'm using, I additionally converted the words into standard tags.\n",
    "\n",
    "Then, I built indexes for single words, or *unigrams*; for two-word phrases, or *bigrams*; for three-word phrases, or *trigrams*; and for four-word phrases, or 4-grams. This allows me to find the raw frequencies of multi-word phrases throughout the document set, as well as to find the percentage of documents containing the phrases.\n",
    "\n",
    "The function I'm using, `get_n_gram_counts`, returns a frequency counter for the combined candidates, for Hickenlooper, and for Gardner, both at the level of each individual document and at the level of each set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_analysis import tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "combined_clean_text = fundraising.clean_text\n",
    "hickenlooper = fundraising[fundraising.candidate == \"hickenlooper\"]\n",
    "hickenlooper_clean_text = hickenlooper.clean_text\n",
    "gardner = fundraising[fundraising.candidate == \"gardner\"]\n",
    "gardner_clean_text = gardner.clean_text\n",
    "\n",
    "def get_n_gram_counts(\n",
    "    baseline_text,\n",
    "    cand1,\n",
    "    cand2,\n",
    "    n=1\n",
    "):\n",
    "    \"\"\"Returns 3 pandas Series and 3 nltk FreqDists from a set of 3 distinct iterators of text.\"\"\"\n",
    "    n_gram = lambda x: tokenize.get_ngrams(x, n=n)\n",
    "    all_tokens = (\n",
    "        baseline_text.apply(tokenize.tokenize_email)\n",
    "        .apply(list)\n",
    "        .apply(n_gram)\n",
    "        .apply(FreqDist)\n",
    "    )\n",
    "    cand1_toks = (\n",
    "        cand1.apply(tokenize.tokenize_email)\n",
    "        .apply(list)\n",
    "        .apply(n_gram)\n",
    "        .apply(FreqDist)\n",
    "    )\n",
    "    cand2_toks = (\n",
    "        cand2.apply(tokenize.tokenize_email)\n",
    "        .apply(list)\n",
    "        .apply(n_gram)\n",
    "        .apply(FreqDist)\n",
    "    )\n",
    "    \n",
    "    return all_tokens, all_tokens.sum(), cand1_toks, cand1_toks.sum(), cand2_toks, cand2_toks.sum()\n",
    "\n",
    "unigram_data = get_n_gram_counts(\n",
    "    combined_clean_text,\n",
    "    gardner_clean_text,\n",
    "    hickenlooper_clean_text,\n",
    "    n=1\n",
    ")\n",
    "bigram_data = get_n_gram_counts(\n",
    "    combined_clean_text,\n",
    "    gardner_clean_text,\n",
    "    hickenlooper_clean_text,\n",
    "    n=2\n",
    ")\n",
    "trigram_data = get_n_gram_counts(\n",
    "    combined_clean_text,\n",
    "    gardner_clean_text,\n",
    "    hickenlooper_clean_text,\n",
    "    n=3\n",
    ")\n",
    "four_gram_data = get_n_gram_counts(\n",
    "    combined_clean_text,\n",
    "    gardner_clean_text,\n",
    "    hickenlooper_clean_text,\n",
    "    n=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "At this point, I began exploring the data. In order to do this, I calculated the `log odds ratio` between language Hickenlooper used and language Gardner used. The *odds* of a candidate using a particular word is simply the probability of finding that word at random within the candidate's emails and dividing by the probability of not finding that word at random. In other words, $Odds_{w, candidate} = \\frac{f_{w, candidate}}{1 - f_{w, candidate}}$, where $f_{w, candidate}$ is the frequency of that word within the candidate's emails.\n",
    "\n",
    "From there, the log-odds ratio is $\\log{Odds_{w, Gardner}} - \\log{Odds_{w, Hickenlooper}}$. (Technically, I added a slight weight to all of the frequencies to avoid divide-by-zero errors.)\n",
    "\n",
    "I decided to do this because of a [2008 paper demonstrating the metric](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf). As that paper notes, there are flaws to this metric. But for exploratory analysis, it has the benefit of being a symmetric measure and being relatively good at finding words that meaningfully distinguish the language of two candidates, as opposed to words like \"and\" or \"the\" that are common among both candidates.\n",
    "\n",
    "For each of the unigram, bigram, trigram, and four-gram datasets, I'll show the 20 words that had the highest and lowest log-odds ratios. When I initially conducted this analysis, I also used a interactive frequency graphic in altair that allowed me to hover over individual words in the dataset. The graphic was inspired from the Monroe paper above and served as a useful tool for exploring and finding words that Hickenlooper or Gardner used at much higher rates than the other.\n",
    "\n",
    "Unfortunately, `altair` does not render well on GitHub. However, I'll show a static example of what those looked like after creating a function for the log-odds ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from email_analysis import display\n",
    "\n",
    "def nonzero_freq(freq_dist, token, combined_dist):\n",
    "    \"\"\"Takes a frequency distribution and a token and returns \n",
    "    (num_occurrences + 1) / (unique_tokens + total_tokens)\n",
    "    \"\"\"\n",
    "    return (freq_dist[token] + 1) / (freq_dist.N() + combined_dist.B())\n",
    "\n",
    "def log_odds_ratio(\n",
    "    token,\n",
    "    all_doc,\n",
    "    all_combined,\n",
    "    gardner_doc,\n",
    "    gardner_combined,\n",
    "    hick_doc,\n",
    "    hick_combined\n",
    "):\n",
    "    freq_gardner = nonzero_freq(gardner_combined, token, all_combined)\n",
    "    freq_hick = nonzero_freq(hick_combined, token, all_combined)\n",
    "    gardner_odds = (freq_gardner / (1 - freq_gardner))\n",
    "    hick_odds = (freq_hick / (1 - freq_hick))\n",
    "    return  math.log(gardner_odds) - math.log(hick_odds)\n",
    "\n",
    "chart = display.display_frequency_graph(log_odds_ratio, *unigram_data)\n",
    "chart.save(\"frequency-chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A scatterplot comparing the frequencies of words to their log-odds ratios](frequency-chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic maps the overall frequencies of the words to their log-odds ratios. In the interactive version, I was able to hover over each point and see the corresponding word. This allowed me to identify words that weren't captured in the lists of top 20 ratios below. The graphic also allowed me to find out the overall frequencies of the words I was looking at, which helped me ensure that the words I was selecting were not so rare that their high log-odds ratios could've come purely from random chance.\n",
    "\n",
    "There was one other intuition I developed with the graphic through exploration: the band of points at the top and bottom of the graphic that are the farthest separated from the rest of the cluster seem to exclusively come from words or phrases that only appear in one candidate's emails. As a result, the graphic also helped me keep a good mix of words that both Gardner and Hickenlooper used in their emails and words that were exclusive to one candidate's emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Word Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Low Scores    \t                              High Scores\n",
      "  1. hick                                     -3.91434\tx                                        6.77604\n",
      "  2. tear                                     -3.91434\tsandra                                   5.76816\n",
      "  3. trial                                    -3.82729\tradical                                  5.73136\n",
      "  4. corporate                                -3.80502\tleft                                     5.02563\n",
      "  5. fundraiser                               -3.75665\tmatch                                    5.02127\n",
      "  6. smith                                    -3.70662\tsocialist                                4.80250\n",
      "  7. witness                                  -3.70662\to                                        4.74574\n",
      "  8. violence                                 -3.68064\tconservative                             4.65679\n",
      "  9. pitch                                    -3.58427\tschumer                                  4.56907\n",
      " 10. mcconnell                                -3.56686\tpatriot                                  4.24832\n",
      " 11. —                                        -3.48251\tsocialism                                4.22053\n",
      " 12. toss                                     -3.47698\too                                       4.22053\n",
      " 13. budget                                   -3.41027\tliberal                                  4.22053\n",
      " 14. nasty                                    -3.41027\tunlock                                   4.20633\n",
      " 15. donald                                   -3.36170\tgenerous                                 4.13218\n",
      " 16. folk                                     -3.34059\tchuck                                    3.95635\n",
      " 17. shady                                    -3.33879\tbernie                                   3.86970\n",
      " 18. mail                                     -3.32032\tbehalf                                   3.78703\n",
      " 19. unprecedented                            -3.30104\thesitate                                 3.74181\n",
      " 20. climate                                  -3.23518\tgrowth                                   3.71916\n"
     ]
    }
   ],
   "source": [
    "display.display_differences(log_odds_ratio, *unigram_data, num_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Word Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Low Scores    \t                              High Scores\n",
      "  1. right wing                               -4.53427\t_<digit>_ x                              6.62144\n",
      "  2. mcconnell '                              -4.44113\tx match                                  6.56530\n",
      "  3. or more                                  -4.38517\t_<links>_ donate                         6.10470\n",
      "  4. , folk                                   -4.17742\t_<prices>_ _<links>_                     5.98739\n",
      "  5. <s> folk                                 -4.14514\tteam gardner                             5.93481\n",
      "  6. hello —                                  -4.12861\t_<links>_ _<digit>_                      5.58104\n",
      "  7. m .                                      -4.09469\tmatch donate                             5.41868\n",
      "  8. this campaign                            -4.08641\tsandra ,                                 5.32409\n",
      "  9. like this                                -4.07729\tmatch _<prices>_                         5.17132\n",
      " 10. trump '                                  -4.02321\t, sandra                                 5.17132\n",
      " 11. corporate pac                            -3.99537\tthe radical                              5.14343\n",
      " 12. the gop                                  -3.97613\tcory s                                   5.07315\n",
      " 13. can afford                               -3.90538\tfar left                                 5.02975\n",
      " 14. mcconnell s                              -3.89522\tcory and                                 4.92288\n",
      " 15. <s> hello                                -3.88432\tx _<digit>_                              4.87967\n",
      " 16. folk ,                                   -3.84083\tthe far                                  4.76270\n",
      " 17. most vulnerable                          -3.79535\t_<prices>_ o                             4.75440\n",
      " 18. team hick                                -3.74771\t_<prices>_ x                             4.63025\n",
      " 19. p .                                      -3.72301\tradical left                             4.61119\n",
      " 20. mcconnell be                             -3.69768\tsandra .                                 4.51003\n"
     ]
    }
   ],
   "source": [
    "display.display_differences(log_odds_ratio, *bigram_data, num_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Low Scores    \t                              High Scores\n",
      "  1. _<prices>_ or more                       -5.05091\t_<digit>_ x match                        6.57181\n",
      "  2. of _<prices>_ or                         -4.44552\t_<links>_ donate _<prices>_              6.06726\n",
      "  3. mcconnell ' s                            -4.43339\t_<links>_ _<digit>_ x                    5.58090\n",
      "  4. flip this senate                         -4.38336\tx match donate                           5.41297\n",
      "  5. <s> donate _<links>_                     -4.18546\tdonate _<prices>_ _<links>_              5.34078\n",
      "  6. </s> <s> folk                            -4.13744\tx match _<prices>_                       5.07442\n",
      "  7. win this election                        -4.10409\t_<prices>_ _<links>_ _<digit>_           5.04344\n",
      "  8. have the resource                        -4.08699\tmatch donate _<prices>_                  4.90890\n",
      "  9. m . e                                    -4.06960\t_<prices>_ x _<digit>_                   4.63771\n",
      " 10. senate seat blue                         -4.05189\tx _<digit>_ _<links>_                    4.60898\n",
      " 11. trump ' s                                -4.01551\tthe radical left                         4.56935\n",
      " 12. mitch mcconnell '                        -3.93853\tdonate _<prices>_ x                      4.55919\n",
      " 13. the resource to                          -3.93853\tthe far left                             4.54893\n",
      " 14. </s> <s> hello                           -3.87664\tnow _<links>_ donate                     4.52809\n",
      " 15. and flip this                            -3.87664\tsandra . </s>                            4.50680\n",
      " 16. you can afford                           -3.87664\t_<prices>_ _<links>_ donate              4.48505\n",
      " 17. and take back                            -3.81067\t, sandra .                               4.47400\n",
      " 18. <s> team ,                               -3.76414\tteam gardner s                           4.41681\n",
      " 19. donate _<prices>_ or                     -3.74004\t_<digit>_ _<links>_ donate               4.41681\n",
      " 20. p . s                                    -3.71534\t_<prices>_ o _<links>_                   4.35616\n"
     ]
    }
   ],
   "source": [
    "display.display_differences(log_odds_ratio, *trigram_data, num_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Low Scores    \t                              High Scores\n",
      "  1. of _<prices>_ or more                    -4.42988\t_<links>_ _<digit>_ x match              5.58416\n",
      "  2. flip this senate seat                    -4.36694\t_<digit>_ x match donate                 5.41626\n",
      "  3. </s> <s> donate _<links>_                -4.18196\t_<digit>_ x match _<prices>_             5.07776\n",
      "  4. _<prices>_ or more to                    -4.10060\t_<prices>_ _<links>_ _<digit>_ x         5.04678\n",
      "  5. . </s> <s> folk                          -4.08350\tx match donate _<prices>_                4.89776\n",
      "  6. m . e .                                  -4.04840\t_<links>_ donate _<prices>_ now          4.83756\n",
      "  7. mitch mcconnell ' s                      -3.93504\tdonate _<prices>_ now _<links>_          4.79801\n",
      "  8. . </s> <s> hello                         -3.87315\tdonate _<prices>_ x _<digit>_            4.56258\n",
      "  9. donation of _<prices>_ or                -3.85164\tnow _<links>_ donate _<prices>_          4.53148\n",
      " 10. have the resource to                     -3.80718\t_<prices>_ now _<links>_ donate          4.52089\n",
      " 11. </s> <s> team ,                          -3.76065\tsandra . </s> <s>                        4.49938\n",
      " 12. and take back the                        -3.68653\tmatch donate _<prices>_ _<links>_        4.46621\n",
      " 13. p . s .                                  -3.68653\t, sandra . </s>                          4.46621\n",
      " 14. contribution of _<prices>_ or            -3.66055\t_<prices>_ _<links>_ donate _<prices>_   4.45491\n",
      " 15. donate _<prices>_ or more                -3.66055\t_<prices>_ x _<digit>_ _<links>_         4.43191\n",
      " 16. . </s> <s> friend                        -3.57830\t_<digit>_ _<links>_ donate _<prices>_    4.40837\n",
      " 17. flip a senate seat                       -3.57830\t_<links>_ donate _<prices>_ x            4.38426\n",
      " 18. _<prices>_ or more before                -3.54931\tx _<digit>_ _<links>_ donate             4.37199\n",
      " 19. . </s> <s> smith                         -3.51945\t_<links>_ donate _<prices>_ _<links>_    4.34698\n",
      " 20. make a donation of                       -3.51945\tdonate _<prices>_ _<links>_ donate       4.28157\n"
     ]
    }
   ],
   "source": [
    "display.display_differences(log_odds_ratio, *four_gram_data, num_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "In TV ads Cory Gardner and, to a lesser extent, John Hickenlooper, have cast themselves as bipartisan candidates. Casting himself as someone who works across the aisle, Gardner's first TV ad showed Gov. Jared Polis citing how consistently the two are in contact during the coronavirus pandemic.\n",
    "\n",
    "\"Senator Cory Gardner, who I talk with multiple times every day, has done everything I've asked to help in our response,\" the ad shows Polis saying.\n",
    "\n",
    "But in line with the political science research, you can see in the log-odds ratios that the candidates have been using partisan language in their emails. \n",
    "\n",
    "### People\n",
    "\n",
    "One place where this particularly comes across is in the political figures the two candidates mention. McConnell and Donald Trump are both disproportionately referenced by Hickenlooper, while Gardner disproportionately mentions Schumer. You can see precise statistics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(word):\n",
    "    ngram = len(word)\n",
    "    phrase = \" \".join(word)\n",
    "    if ngram == 1:\n",
    "        dataset = unigram_data\n",
    "    elif ngram == 2:\n",
    "        dataset = bigram_data\n",
    "    elif ngram == 3:\n",
    "        dataset = trigram_data\n",
    "    elif ngram == 4:\n",
    "        dataset = four_gram_data\n",
    "    else:\n",
    "        raise ValueError(\"The word is too long/short\")\n",
    "    gardner_pct = dataset[2].apply(lambda x: word in x).mean()\n",
    "    hick_pct = dataset[4].apply(lambda x: word in x).mean()\n",
    "    print(f\"Gardner mentioned {phrase} in {gardner_pct * 100.} percent of his e-mails\")\n",
    "    print(f\"Gardner mentioned {phrase} in {dataset[2].apply(lambda x: word in x).sum()} of his {len(dataset[2])} emails\")\n",
    "    print(f\"Hickenlooper mentioned {phrase} in {hick_pct * 100.} percent of his e-mails\")\n",
    "    print(f\"Hickenlooper mentioned {phrase} in {dataset[4].apply(lambda x: word in x).sum()} of his {len(dataset[4])} emails\")\n",
    "    if word in dataset[3] and word in dataset[5]:\n",
    "        greater_cand = \"Gardner\" if gardner_pct > hick_pct else \"Hickenlooper\"\n",
    "        lesser_cand = \"Hickenlooper\" if gardner_pct > hick_pct else \"Gardner\"\n",
    "        min_num = min(gardner_pct, hick_pct)\n",
    "        max_num = max(gardner_pct, hick_pct)\n",
    "        odds_ratio = (max_num / (1. - max_num)) / (min_num / (1. - min_num))\n",
    "        print(f\"The odds of {greater_cand} using the word {phrase} were {odds_ratio} times higher than {lesser_cand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuck Schumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned schumer in 36.56957928802589 percent of his e-mails\n",
      "Gardner mentioned schumer in 113 of his 309 emails\n",
      "Hickenlooper mentioned schumer in 0.26666666666666666 percent of his e-mails\n",
      "Hickenlooper mentioned schumer in 1 of his 375 emails\n",
      "The odds of Gardner using the word schumer were 215.6224489795918 times higher than Hickenlooper\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"schumer\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned chuck schumer in 19.41747572815534 percent of his e-mails\n",
      "Gardner mentioned chuck schumer in 60 of his 309 emails\n",
      "Hickenlooper mentioned chuck schumer in 0.26666666666666666 percent of his e-mails\n",
      "Hickenlooper mentioned chuck schumer in 1 of his 375 emails\n",
      "The odds of Gardner using the word chuck schumer were 90.12048192771084 times higher than Hickenlooper\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"chuck\", \"schumer\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mitch McConnell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned mitch mcconnell in 2.26537216828479 percent of his e-mails\n",
      "Gardner mentioned mitch mcconnell in 7 of his 309 emails\n",
      "Hickenlooper mentioned mitch mcconnell in 47.199999999999996 percent of his e-mails\n",
      "Hickenlooper mentioned mitch mcconnell in 177 of his 375 emails\n",
      "The odds of Hickenlooper using the word mitch mcconnell were 38.56709956709956 times higher than Gardner\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"mitch\", \"mcconnell\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned mcconnell in 2.26537216828479 percent of his e-mails\n",
      "Gardner mentioned mcconnell in 7 of his 309 emails\n",
      "Hickenlooper mentioned mcconnell in 55.733333333333334 percent of his e-mails\n",
      "Hickenlooper mentioned mcconnell in 209 of his 375 emails\n",
      "The odds of Hickenlooper using the word mcconnell were 54.318416523235804 times higher than Gardner\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"mcconnell\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned donald trump in 0.6472491909385114 percent of his e-mails\n",
      "Gardner mentioned donald trump in 2 of his 309 emails\n",
      "Hickenlooper mentioned donald trump in 28.000000000000004 percent of his e-mails\n",
      "Hickenlooper mentioned donald trump in 105 of his 375 emails\n",
      "The odds of Hickenlooper using the word donald trump were 59.69444444444445 times higher than Gardner\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"donald\", \"trump\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partisan Signals\n",
    "\n",
    "You can similarly see a number of partisan symbols in the data, with Gardner making references to the \"far-left\" and to \"socialists\" and Hickenlooper referring to \"special interests\" and to \"billionaires\" — a theme a political scientist told me is popular in political emails among Democrats in the Senate as a whole.\n",
    "\n",
    "#### Socialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned socialist in 26.537216828478964 percent of his e-mails\n",
      "Gardner mentioned socialist in 82 of his 309 emails\n",
      "Hickenlooper mentioned socialist in 0.0 percent of his e-mails\n",
      "Hickenlooper mentioned socialist in 0 of his 375 emails\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"socialist\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned radical in 55.663430420711975 percent of his e-mails\n",
      "Gardner mentioned radical in 172 of his 309 emails\n",
      "Hickenlooper mentioned radical in 0.0 percent of his e-mails\n",
      "Hickenlooper mentioned radical in 0 of his 375 emails\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"radical\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Far-left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned far left in 33.980582524271846 percent of his e-mails\n",
      "Gardner mentioned far left in 105 of his 309 emails\n",
      "Hickenlooper mentioned far left in 0.0 percent of his e-mails\n",
      "Hickenlooper mentioned far left in 0 of his 375 emails\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"far\", \"left\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned special interest in 12.944983818770226 percent of his e-mails\n",
      "Gardner mentioned special interest in 40 of his 309 emails\n",
      "Hickenlooper mentioned special interest in 17.599999999999998 percent of his e-mails\n",
      "Hickenlooper mentioned special interest in 66 of his 375 emails\n",
      "The odds of Hickenlooper using the word special interest were 1.4364077669902913 times higher than Gardner\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"special\", \"interest\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Billionaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned billionaire in 1.6181229773462782 percent of his e-mails\n",
      "Gardner mentioned billionaire in 5 of his 309 emails\n",
      "Hickenlooper mentioned billionaire in 11.200000000000001 percent of his e-mails\n",
      "Hickenlooper mentioned billionaire in 42 of his 375 emails\n",
      "The odds of Hickenlooper using the word billionaire were 7.668468468468468 times higher than Gardner\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"billionaire\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References to Money\n",
    "\n",
    "You can also see that both candidates made frequent references to money. And you can see that Cory Gardner made frequent references to matching — something that did not appear to come up in Hickenlooper's emails at all. Campaign finance experts [have questioned how these campaigns can work](https://www.opensecrets.org/news/2019/08/political-contributions-campaigns-say-theyll-match/).\n",
    "\n",
    "I've analyzed both of the references to money based on regular expression. I matched references to prices (e.g. $100.00) using this reasonable regular expression from `CommonRegex`:\n",
    "\n",
    "`'[$]\\s?[+-]?[0-9]{1,3}(?:(?:,?[0-9]{3}))*(?:\\.[0-9]{1,2})?'`\n",
    "\n",
    "For matching digits, I used a simple `[0-9]+` regular expression. I tagged items by digits last, after prices, phone numbers and a few other things.\n",
    "\n",
    "#### Prices\n",
    "\n",
    "Both candidates made frequent references to money in their emails, something that a political scientist, Taewoo Kang, told me doesn't happen on TV at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned _<prices>_ in 70.22653721682848 percent of his e-mails\n",
      "Gardner mentioned _<prices>_ in 217 of his 309 emails\n",
      "Hickenlooper mentioned _<prices>_ in 62.4 percent of his e-mails\n",
      "Hickenlooper mentioned _<prices>_ in 234 of his 375 emails\n",
      "The odds of Gardner using the word _<prices>_ were 1.4212653288740247 times higher than Hickenlooper\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"_<prices>_\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Campaigns\n",
    "\n",
    "And, as mentioned before, Gardner uses matching campaigns regularly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned _<digit>_ x match in 38.51132686084142 percent of his e-mails\n",
      "Gardner mentioned _<digit>_ x match in 119 of his 309 emails\n",
      "Hickenlooper mentioned _<digit>_ x match in 0.0 percent of his e-mails\n",
      "Hickenlooper mentioned _<digit>_ x match in 0 of his 375 emails\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"_<digit>_\", \"x\", \"match\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vulnerable Republican Senator\n",
    "\n",
    "Another point that I found interesting on the money front was how the candidates discussed their opponents. If you read through examples of these phrases, you could see Hickenlooper often bringing McConnell up as a way of referencing the moneyed interests in the Republican Party. And Gardner often warned of the far-left as he tried to get people to give him money. As Tyler Sandberg, a GOP consultant, told me, \"In fundraising, it's very much, 'The sky is falling, give now, save the world.'\"\n",
    "\n",
    "On this front, Hickenlooper mentioned Gardner's political vulnerability quite often as pointed to the importance of winning the race. He similarly made frequent references to \"flipping this Senate seat blue\" or variants of that phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned vulnerable republican senator in 0.0 percent of his e-mails\n",
      "Gardner mentioned vulnerable republican senator in 0 of his 309 emails\n",
      "Hickenlooper mentioned vulnerable republican senator in 8.0 percent of his e-mails\n",
      "Hickenlooper mentioned vulnerable republican senator in 30 of his 375 emails\n"
     ]
    }
   ],
   "source": [
    "display_stats((\"vulnerable\", \"republican\", \"senator\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impeachment\n",
    "\n",
    "The final interesting point — again, related to the partisan language of campaign emails — lies in how often the candidates talked about impeachment. In order to assess this, I looked at how often the candidates used any one of a set of impeachment-related words. This, of course, is not conclusive of their language, but tracks well with what I seemed to be seeing when I read the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned one of trial, impeachment, acquit, witness, impeach, ukraine in 0.6472491909385114 percent of his e-mails\n",
      "Hickenlooper mentioned one of trial, impeachment, acquit, witness, impeach, ukraine in 9.866666666666667 percent of his e-mails\n",
      "Hickenlooper mentioned trial, impeachment, acquit, witness, impeach, ukraine 81.00634190847614 times more frequently than Gardner\n"
     ]
    }
   ],
   "source": [
    "def impeachment(dataset, any_words):\n",
    "    phrase = ', '.join([\"\".join(w) for w in any_words])\n",
    "    in_impeachment = lambda x: any([word in x for word in any_words])\n",
    "    gardner_pct = dataset[2].apply(in_impeachment).mean() * 100.\n",
    "    hick_pct = dataset[4].apply(in_impeachment).mean() * 100.\n",
    "    print(f\"Gardner mentioned one of {phrase} in {gardner_pct} percent of his e-mails\")\n",
    "    print(f\"Hickenlooper mentioned one of {phrase} in {hick_pct} percent of his e-mails\")\n",
    "    freq_gardner = sum([dataset[3].freq(word) for word in any_words])\n",
    "    freq_hick = sum([dataset[5].freq(word) for word in any_words])\n",
    "    if freq_gardner != 0 and freq_hick != 0:\n",
    "        if freq_hick > freq_gardner:\n",
    "            print(f\"Hickenlooper mentioned {phrase} {freq_hick / freq_gardner} times more frequently than Gardner\")\n",
    "        else:\n",
    "            print(f\"Gardner mentioned {phrase} {freq_gardner / freq_hick} times more frequently than Hickenlooper\")\n",
    "\n",
    "impeachment_words = [\n",
    "    (\"trial\",), (\"impeachment\",),\n",
    "    (\"acquit\",), (\"witness\",),\n",
    "    (\"impeach\",), (\"ukraine\",)\n",
    "]        \n",
    "impeachment(unigram_data, impeachment_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see a stark contrast here. But you really see the whole contrast if you narrow your search to January and February, when the hearings were going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardner mentioned one of trial, impeachment, acquit, witness, impeach, ukraine in 0.0 percent of his e-mails\n",
      "Hickenlooper mentioned one of trial, impeachment, acquit, witness, impeach, ukraine in 40.32258064516129 percent of his e-mails\n"
     ]
    }
   ],
   "source": [
    "all_janfeb = fundraising[\n",
    "    (fundraising.date >= datetime.datetime(2020, 1, 1, tzinfo=pytz.timezone(COLORADO))) &\n",
    "    (fundraising.date < datetime.datetime(2020, 3, 1, tzinfo=pytz.timezone(COLORADO)))\n",
    "]\n",
    "hick_janfeb = all_janfeb[all_janfeb.candidate == \"hickenlooper\"]\n",
    "gardner_janfeb = all_janfeb[all_janfeb.candidate == \"gardner\"]\n",
    "jf_unigrams = get_n_gram_counts(\n",
    "    all_janfeb.clean_text,\n",
    "    gardner_janfeb.clean_text,\n",
    "    hick_janfeb.clean_text,\n",
    "    n=1\n",
    ")\n",
    "impeachment(jf_unigrams, impeachment_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The data shows that the candidates are messaging themselves in particularly partisan ways in emails, something that reflects how different the audiences of political emails are from the audiences of, say, television ads. Particularly in a race where Sen. Gardner has made such an emphasis of his bipartisan record, this seems notable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email-analysis",
   "language": "python",
   "name": "email-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
